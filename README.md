# Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs

[![arXiv](https://img.shields.io/badge/arXiv-2507.xxxxx-b31b1b.svg)](https://arxiv.org/abs/2507.xxxxx)

This repository contains the official implementation of "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs".

## Overview

We are currently organizing and preparing to release all code and resources used in our paper. This repository will include:

- Implementation of SAE-guided supervised finetuning method
- Code for analyzing code-switching behaviors in LLMs

## Installation

```bash
git clone https://github.com/username/sae-codeswitching.git
cd SASFT
pip install -r requirements.txt
```

## Coming Soon

We are actively working on releasing:

- [ ] Complete implementation of SAE-guided finetuning pipeline
- [ ] Code-switching analysis tools
- [ ] Detailed documentation and usage examples


## Contact

For questions about our code release or paper, please contact Boyi Deng at [dengboyi@mail.ustc.edu.cn].

Stay tuned for updates as we prepare our code release!

Note: This is a preliminary README that will be updated with more detailed information when the code is released.
